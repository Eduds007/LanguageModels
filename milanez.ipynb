{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3158.36it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Generating train split: 307373 examples [02:19, 2210.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#carrega natural questions\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=\"simplified-nq-train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 14:02:15.199794: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 14:02:15.422451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 14:02:16.218130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#carrega modelo de tradução\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['title', 'context','question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = sample['document_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/eduardo/Área de Trabalho/4_semestre/IC/LanguageModels/milanez.ipynb Célula 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eduardo/%C3%81rea%20de%20Trabalho/4_semestre/IC/LanguageModels/milanez.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?<=<h1>)(.+?)(?=</h1>)\u001b[39m\u001b[39m'\u001b[39m, titulo, re\u001b[39m.\u001b[39mIGNORECASE)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "re.findall(r'(?<=<h1>)(.+?)(?=</h1>)', titulo, re.IGNORECASE)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'yes_no_answer': 'NONE',\n",
       "  'long_answer': {'start_token': 1952,\n",
       "   'candidate_index': 54,\n",
       "   'end_token': 2019},\n",
       "  'short_answers': [{'start_token': 1960, 'end_token': 1969}],\n",
       "  'annotation_id': 5.931654502200276e+17}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sample['document_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_token': 1952, 'candidate_index': 54, 'end_token': 2019}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['annotations'][0]['long_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = sample['annotations'][0]['short_answers'][0]['start_token']\n",
    "end = sample['annotations'][0]['short_answers'][0]['end_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a newsletter sent to an advertising firm 's customers\""
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(doc.split(\" \")[init:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what happened to the lost settlement of roanoke'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample['question_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 100.0%99999999999%%"
     ]
    }
   ],
   "source": [
    "def translate(size=10, dataset=dataset):\n",
    "    df = pd.DataFrame(columns = ['title', 'question', 'long_answer', 'short_answer'])\n",
    "    small_set = dataset['train']\n",
    "    for i in range(size):\n",
    "\n",
    "        doc = re.sub(r'''<(?:\"[^\"]*\"['\"]*|'[^']*'['\"]*|[^'\">])+>''','',small_set[i]['document_text'])\n",
    "        init_long = small_set[i]['annotations'][0]['long_answer']['start_token']\n",
    "        end_long = small_set[i]['annotations'][0]['long_answer']['end_token']\n",
    "        if len(small_set[i]['annotations'][0]['short_answers']) != 0:\n",
    "            init_short = small_set[i]['annotations'][0]['short_answers'][0]['start_token']\n",
    "            end_short = small_set[i]['annotations'][0]['short_answers'][0]['end_token']\n",
    "        else:\n",
    "            init_short = 0\n",
    "            end_short  = 0\n",
    "        question = small_set[i]['question_text']\n",
    "        long_answer = \" \".join(doc.split(\" \")[init_long:end_long])\n",
    "        short_answer = \" \".join(doc.split(\" \")[init_short:end_short])\n",
    "        title = re.findall(r'(?<=<h1>)(.+?)(?=</h1>)', small_set[i]['document_text'], re.IGNORECASE)[0]\n",
    "        #if len(long_answer) > 0:  \n",
    "            #title = pipe('>>por<< ' + title)[0]['translation_text']\n",
    "            #long_answer= pipe('>>por<< ' + long_answer)[0]['translation_text']\n",
    "            #if len(short_answer) != 0:\n",
    "            #    short_answer= pipe('>>por<< ' + short_answer)[0]['translation_text']\n",
    "            #question= pipe('>>por<< ' + question)[0]['translation_text']\n",
    "\n",
    "            \n",
    "            \n",
    "        #else:\n",
    "        #    pass\n",
    "            \n",
    "        new_row = {'title': [title], 'question': [question], 'long_answer': [long_answer], 'short_answer': [short_answer]}\n",
    "        df_aux = pd.DataFrame(new_row)\n",
    "        df = pd.concat([df, df_aux])\n",
    "\n",
    "        #configuração do terminal\n",
    "        sys.stdout.write(f'\\rProgress : {((i+1)/size)*100}%')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    df = df.loc[df['long_answer'] != '']\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df\n",
    "    return df\n",
    "    \n",
    "translate(1000, dataset=dataset).to_excel('saida.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress : 100.0%00000000001%Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "traduzido = translate(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = {\n",
    "    \"version\": \"0.1\",\n",
    "    \"data\": [\n",
    "        \n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for passage in json.loads(traduzido.to_json(orient='records')):\n",
    "    estrutura = {\n",
    "        \"title\": passage['title'],\n",
    "        \"paragraphs\": [\n",
    "            {\n",
    "            \"context\": passage['long_answer'],\n",
    "            \"qas\": [\n",
    "                {\n",
    "                    \"id\":0,\n",
    "                    \"question\": passage['question'],\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                        \"answer_start\":0,\n",
    "                        'text':passage['short_answer'],\n",
    "                        }\n",
    "                        \n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    squad['data'].append(estrutura)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable saved as JSON\n"
     ]
    }
   ],
   "source": [
    "json_data =squad\n",
    "file_path = \"data.json\"\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Variable saved as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira linha da planilha:\n",
      "Unnamed: 0                                                      0\n",
      "title                                            Email marketing \n",
      "question        which is the most common use of opt-in e-mail ...\n",
      "long_answer      A common example of permission marketing is a...\n",
      "short_answer    a newsletter sent to an advertising firm 's cu...\n",
      "t                                              Marketing de email\n",
      "q               que é o uso mais comum do marketing de e-mail ...\n",
      "l                Um exemplo comum de marketing de permissão é ...\n",
      "s               Um boletim informativo enviado aos clientes de...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Excel spreadsheet\n",
    "planilha = pd.read_excel('res.xlsx')\n",
    "\n",
    "print(\"Primeira linha da planilha:\")\n",
    "print(planilha.iloc[0])\n",
    "\n",
    "# Initialize the JSON structure\n",
    "squad = {\"data\": []}\n",
    "\n",
    "# Iterate over the rows of the spreadsheet\n",
    "for idx, linha in planilha.iterrows():\n",
    "    estrutura = {\n",
    "        \"title\": str(linha['t']),  # Assuming there is a column named 't' for title\n",
    "        \"paragraphs\": [\n",
    "            {\n",
    "                \"context\": str(linha['q']),  # Replace 'q' with the correct column name for the question\n",
    "                \"qas\": [\n",
    "                    {\n",
    "                        \"id\": idx,\n",
    "                        \"question\": str(linha['l']),  # Replace 's' with the correct column name for the answer\n",
    "                        \"answers\": [\n",
    "                            {\n",
    "                                \"answer_start\": 0,\n",
    "                                \"text\": str(linha['s']),\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    squad['data'].append(estrutura)\n",
    "\n",
    "# Save the JSON to a file\n",
    "with open('googleTrad2.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(squad, json_file, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
